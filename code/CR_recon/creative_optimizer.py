"""
Creative Optimizer: í›ˆë ¨ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ê³  ì°½ì˜ì ì¸ ëª¨ë¸ê³¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œì•ˆ
ì‚¬ìš©: python creative_optimizer.py --log outputs/train_log.txt
"""
import argparse
import re
import yaml
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any
import subprocess


def parse_train_log(log_path: str) -> Dict[str, Any]:
    """í›ˆë ¨ ë¡œê·¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì„¤ì •ê³¼ ê²°ê³¼ ì¶”ì¶œ"""
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()

    result = {
        'model_name': None,
        'model_path': None,
        'model_params': {},
        'loss_name': None,
        'loss_path': None,
        'loss_params': {},
        'hyperparams': {},
        'best_val_loss': float('inf'),
        'final_val_loss': float('inf'),
        'epochs_trained': 0,
        'train_losses': [],
        'val_losses': [],
        'raw_content': content,
    }

    # Model ì •ë³´ ì¶”ì¶œ
    model_match = re.search(r'Model: (\w+)', content)
    if model_match:
        result['model_name'] = model_match.group(1)

    model_path_match = re.search(r'Model Path: ([\w/._-]+)', content)
    if model_path_match:
        result['model_path'] = model_path_match.group(1)

    # Model Parameters ì¶”ì¶œ
    model_params_section = re.search(
        r'Model Parameters:(.*?)(?=Loss Function|Hyperparameters)',
        content,
        re.DOTALL
    )
    if model_params_section:
        params_text = model_params_section.group(1)
        for line in params_text.split('\n'):
            match = re.search(r'-\s*(\w+):\s*(.*)', line)
            if match:
                param_name = match.group(1)
                param_value = match.group(2).strip()
                try:
                    result['model_params'][param_name] = yaml.safe_load(param_value)
                except:
                    result['model_params'][param_name] = param_value

    # Loss ì •ë³´ ì¶”ì¶œ
    loss_match = re.search(r'Loss Function: (\w+)', content)
    if loss_match:
        result['loss_name'] = loss_match.group(1)

    loss_path_match = re.search(r'Loss Path: ([\w/._-]+)', content)
    if loss_path_match:
        result['loss_path'] = loss_path_match.group(1)

    # Loss Parameters ì¶”ì¶œ
    loss_params_section = re.search(
        r'Loss Parameters:(.*?)(?=Hyperparameters)',
        content,
        re.DOTALL
    )
    if loss_params_section:
        params_text = loss_params_section.group(1)
        for line in params_text.split('\n'):
            match = re.search(r'-\s*(\w+):\s*(.*)', line)
            if match:
                param_name = match.group(1)
                param_value = match.group(2).strip()
                try:
                    result['loss_params'][param_name] = yaml.safe_load(param_value)
                except:
                    result['loss_params'][param_name] = param_value

    # Hyperparameters ì¶”ì¶œ
    hyper_section = re.search(
        r'Hyperparameters:(.*?)(?=Data Statistics)',
        content,
        re.DOTALL
    )
    if hyper_section:
        hyper_text = hyper_section.group(1)
        for line in hyper_text.split('\n'):
            match = re.search(r'-\s*([\w\s]+):\s*(.*)', line)
            if match:
                param_name = match.group(1).strip()
                param_value = match.group(2).strip()
                try:
                    result['hyperparams'][param_name] = yaml.safe_load(param_value)
                except:
                    result['hyperparams'][param_name] = param_value

    # Best val loss ì¶”ì¶œ (ëª¨ë“  epochì˜ ì†ì‹¤ê°’ ìˆ˜ì§‘)
    best_loss_matches = re.findall(
        r'best_val=([\d.e+-]+)',
        content
    )
    if best_loss_matches:
        result['best_val_loss'] = float(best_loss_matches[-1])
        result['final_val_loss'] = float(best_loss_matches[-1])
        result['val_losses'] = [float(x) for x in best_loss_matches]

    # Train losses ì¶”ì¶œ
    train_loss_matches = re.findall(
        r'train_loss=([\d.e+-]+)',
        content
    )
    if train_loss_matches:
        result['train_losses'] = [float(x) for x in train_loss_matches]

    # Epochs trained ì¶”ì¶œ
    epoch_matches = re.findall(
        r'\[EPOCH\]\s+(\d+)/(\d+)',
        content
    )
    if epoch_matches:
        result['epochs_trained'] = int(epoch_matches[-1][0])

    return result


def analyze_performance(log_data: Dict[str, Any]) -> Dict[str, Any]:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  ë°©í–¥ íŒŒì•…"""
    analysis = {
        'current_best_loss': log_data['best_val_loss'],
        'final_loss': log_data['final_val_loss'],
        'epochs_trained': log_data['epochs_trained'],
        'model_name': log_data['model_name'],
        'loss_name': log_data['loss_name'],
        'metrics': {}
    }

    # ì†ì‹¤ ê°ì†Œ ì¶”ì´ ë¶„ì„
    if len(log_data['val_losses']) > 1:
        recent_losses = log_data['val_losses'][-min(10, len(log_data['val_losses'])):]
        first_loss = log_data['val_losses'][0]
        last_loss = log_data['val_losses'][-1]

        improvement = (first_loss - last_loss) / first_loss if first_loss > 0 else 0
        analysis['metrics']['improvement_rate'] = improvement
        analysis['metrics']['convergence_trend'] = 'improving' if last_loss < recent_losses[0] else 'plateauing'

    # ê³¼ì í•© ì—¬ë¶€ í™•ì¸
    if len(log_data['train_losses']) > 0 and len(log_data['val_losses']) > 0:
        train_mean = sum(log_data['train_losses']) / len(log_data['train_losses'])
        val_mean = sum(log_data['val_losses']) / len(log_data['val_losses'])
        gap = (val_mean - train_mean) / train_mean if train_mean > 0 else 0
        analysis['metrics']['train_val_gap'] = gap
        analysis['metrics']['overfitting'] = 'likely' if gap > 0.2 else 'moderate' if gap > 0.1 else 'unlikely'

    return analysis


def propose_creative_models(log_data: Dict[str, Any], analysis: Dict[str, Any]) -> List[Dict]:
    """ì°½ì˜ì ì¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì œì•ˆ"""
    proposals = []
    current_model = log_data['model_name']
    current_loss = log_data['best_val_loss']

    # ì œì•ˆ 1: ë§Œì•½ CNN_XAttnì„ ì‚¬ìš© ì¤‘ì´ë©´ â†’ Attention-Enhanced CNN
    if current_model == 'cnn_xattn':
        proposals.append({
            'type': 'model',
            'name': 'MultiHeadChannelAttention',
            'description': 'Multi-head channel attentionì„ ì¶”ê°€í•œ ê°œì„ ëœ CNN ì•„í‚¤í…ì²˜',
            'rationale': 'Transformerì˜ Multi-head mechanismì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì±„ë„ íŠ¹ì„± í•™ìŠµ',
            'architecture_sketch': {
                'stem': 'ë™ì¼ (5x5 conv, stride=2)',
                'backbone': 'Residual blocks with Multi-Head Channel Attention (4 heads)',
                'decoder': 'ê°„ë‹¨í•œ feedforward ëŒ€ì‹  Residual connectionsìœ¼ë¡œ ê°•í™”',
            },
            'expected_benefit': 'attention ë©”ì»¤ë‹ˆì¦˜ ê°•í™”ë¡œ íŠ¹ì„± ì¶”ì¶œ í–¥ìƒ',
            'implementation_file': 'models/multihead_attention_cnn.py',
            'config_changes': {'model': {'name': 'multihead_attention_cnn', 'params': {'heads': 4, 'head_dim': 32}}}
        })

    # ì œì•ˆ 2: ë§Œì•½ ê³¼ì í•©ì´ ì‹¬í•˜ë©´ â†’ Regularized Model
    if analysis['metrics'].get('overfitting') == 'likely':
        proposals.append({
            'type': 'model',
            'name': 'RegularizedCNN',
            'description': 'Batch normalization, Layer normalization, Attention dropoutì„ ê°•í™”í•œ ëª¨ë¸',
            'rationale': 'ê³¼ì í•© ì¦ìƒ â†’ ì •ê·œí™” ë©”ì»¤ë‹ˆì¦˜ ê°•í™” í•„ìš”',
            'architecture_sketch': {
                'normalization': 'GroupNorm â†’ LayerNorm + GroupNorm í˜¼í•©',
                'dropout': 'Spatial dropout ê°•í™”',
                'attention': 'Attention weight ì •ê·œí™” ì¶”ê°€',
            },
            'expected_benefit': 'ê³¼ì í•© ê°ì†Œ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ',
            'implementation_file': 'models/regularized_cnn.py',
            'config_changes': {
                'model': {
                    'name': 'regularized_cnn',
                    'params': {
                        'cnn_dropout': 0.1,
                        'attention_dropout': 0.15,
                        'use_layer_norm': True
                    }
                }
            }
        })

    # ì œì•ˆ 3: ìˆ˜ë ´ ì •ì²´ë˜ì—ˆë‹¤ë©´ â†’ Residual Path ê°•í™”
    if analysis['metrics'].get('convergence_trend') == 'plateauing' and current_loss > 0.01:
        proposals.append({
            'type': 'model',
            'name': 'DenseResidualCNN',
            'description': 'DenseNet ìŠ¤íƒ€ì¼ì˜ Dense residual connections ì¶”ê°€',
            'rationale': 'Gradient flow ê°œì„  ë° íŠ¹ì„± ì¬ì‚¬ìš©ìœ¼ë¡œ ìˆ˜ë ´ ê°€ì†í™”',
            'architecture_sketch': {
                'connections': ' ê° stageì—ì„œ ì´ì „ feature mapsë¥¼ concatenate',
                'bottleneck': 'Channel reductionì„ í†µí•œ íš¨ìœ¨ì„± ìœ ì§€',
                'pooling': 'Dense ì—°ê²°ë¡œ spatial dimension ìœ ì§€',
            },
            'expected_benefit': 'ë” ê¹Šì€ í•™ìŠµ ê²½ë¡œì™€ ë¹ ë¥¸ ìˆ˜ë ´',
            'implementation_file': 'models/dense_residual_cnn.py',
            'config_changes': {
                'model': {
                    'name': 'dense_residual_cnn',
                    'params': {'use_dense_connections': True, 'bottleneck_ratio': 0.5}
                }
            }
        })

    # ì œì•ˆ 4: Hybrid Approach - CNN + Local Transformer
    proposals.append({
        'type': 'model',
        'name': 'LocalTransformerCNN',
        'description': 'Local window attention (íš¨ìœ¨ì )ì„ ì‚¬ìš©í•œ CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ',
        'rationale': 'Full self-attention ë¹„ìš© ì œê±°í•˜ë©° long-range dependencies í•™ìŠµ',
        'architecture_sketch': {
            'blocks': '8x8 local windowsì—ì„œë§Œ attention ìˆ˜í–‰',
            'efficiency': 'Quadratic attention â†’ Linear complexity',
            'fusion': 'CNN featuresë¥¼ local transformerë¡œ ì •ì œ',
        },
        'expected_benefit': 'ë¹ ë¥¸ í•™ìŠµì†ë„ + attentionì˜ ì¥ì  í™œìš©',
        'implementation_file': 'models/local_transformer_cnn.py',
        'config_changes': {
            'model': {
                'name': 'local_transformer_cnn',
                'params': {'window_size': 8, 'num_heads': 4}
            }
        }
    })

    # ì œì•ˆ 5: Spectral êµ¬ì¡° í™œìš© - Fourier Features
    proposals.append({
        'type': 'model',
        'name': 'SpectralAwareCNN',
        'description': 'ì…ë ¥ì˜ spectral íŠ¹ì„±ì„ í™œìš©í•œ ì£¼ê¸°ì„± ì¸ì‹ CNN',
        'rationale': '128x128ì€ ì •ê¸°ì  êµ¬ì¡° â†’ Fourier spaceì—ì„œì˜ ì „ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€',
        'architecture_sketch': {
            'frontend': 'ì…ë ¥ì— FFT ì¶”ê°€ (í•™ìŠµ ê°€ëŠ¥í•œ ì£¼íŒŒìˆ˜ í•„í„°)',
            'frequency_encoding': 'Positional encoding with frequency information',
            'backbone': 'í‘œì¤€ CNN backbone',
        },
        'expected_benefit': 'ì£¼ê¸°ì  êµ¬ì¡° ëª…ì‹œì  í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ',
        'implementation_file': 'models/spectral_aware_cnn.py',
        'config_changes': {
            'model': {
                'name': 'spectral_aware_cnn',
                'params': {'use_fft_encoding': True, 'fft_bins': 32}
            }
        }
    })

    return proposals


def propose_creative_losses(log_data: Dict[str, Any], analysis: Dict[str, Any]) -> List[Dict]:
    """ì°½ì˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ ì œì•ˆ"""
    proposals = []
    current_loss = log_data['loss_name']
    current_val_loss = log_data['best_val_loss']

    # ì œì•ˆ 1: Huber Loss + Correlation (robust + structure)
    proposals.append({
        'type': 'loss',
        'name': 'HuberPearsonLoss',
        'description': 'Huber loss (outlier robust)ì™€ Pearson correlation (êµ¬ì¡° í•™ìŠµ) ê²°í•©',
        'rationale': 'í˜„ì¬ MSE_Pearsonì—ì„œ MSEë¥¼ Huberë¡œ êµì²´í•˜ì—¬ outlierì— ëœ ë¯¼ê°',
        'formula': 'L = 0.8 * Huber(pred, target) + 0.2 * (1 - Pearson correlation)',
        'expected_benefit': 'Outliersì— robustí•˜ë©´ì„œë„ spectral shape ìœ ì§€',
        'implementation_file': 'losses/huber_pearson.py',
        'config_changes': {
            'loss': {
                'name': 'huber_pearson',
                'params': {'huber_delta': 0.5, 'pearson_weight': 0.2}
            }
        }
    })

    # ì œì•ˆ 2: TV(Total Variation) + MSE - Smooth predictions
    proposals.append({
        'type': 'loss',
        'name': 'SmoothnessMSELoss',
        'description': 'MSE + Total Variation (ì¸ì ‘ bin ê°„ì˜ ì°¨ì´ ìµœì†Œí™”)',
        'rationale': 'ìŠ¤í™íŠ¸ëŸ¼ì˜ ë¶€ë“œëŸ¬ìš´ ë³€í™” ê°•ì œë¡œ ë…¸ì´ì¦ˆ ê°ì†Œ ë° ì¼ë°˜í™” í–¥ìƒ',
        'formula': 'L = 0.7 * MSE(pred, target) + 0.3 * TV(pred)',
        'tv_definition': 'ì¸ì ‘ binsì˜ ì ˆëŒ€ê°’ ì°¨ì´ì˜ í•©',
        'expected_benefit': 'ëœ noisyí•œ ìŠ¤í™íŠ¸ëŸ¼ ì˜ˆì¸¡, ë” ë¶€ë“œëŸ¬ìš´ ê³¡ì„ ',
        'implementation_file': 'losses/smoothness_mse.py',
        'config_changes': {
            'loss': {
                'name': 'smoothness_mse',
                'params': {'mse_weight': 0.7, 'tv_weight': 0.3}
            }
        }
    })

    # ì œì•ˆ 3: Quantile Loss - íŠ¹ì • ë²”ìœ„ ê°•ì¡°
    proposals.append({
        'type': 'loss',
        'name': 'QuantileRobustLoss',
        'description': 'í•˜ìœ„ 50%ì™€ ìƒìœ„ 10%ë¥¼ ë‹¤ë¥´ê²Œ ê°€ì¤‘ì¹˜ì£¼ëŠ” Quantile loss',
        'rationale': 'ë‚®ì€ ê°’ì˜ ì •í™•ì„±ì€ ëœ ì¤‘ìš”í•˜ê³  ë†’ì€ ê°’ì€ ë” ì •í™•íˆ',
        'formula': 'L = quantile_loss(pred, target, quantile=0.5) + 2*quantile_loss(pred, target, quantile=0.9)',
        'expected_benefit': 'ì‹¤ì œ ì¤‘ìš”í•œ ë¶€ë¶„(ë†’ì€ intensity)ì— ë” ì§‘ì¤‘',
        'implementation_file': 'losses/quantile_robust.py',
        'config_changes': {
            'loss': {
                'name': 'quantile_robust',
                'params': {'lower_quantile': 0.5, 'upper_quantile': 0.9, 'weight_ratio': 2.0}
            }
        }
    })

    # ì œì•ˆ 4: Wasserstein Loss - Distribution matching
    if current_val_loss > 0.02:
        proposals.append({
            'type': 'loss',
            'name': 'WassersteinSpectralLoss',
            'description': 'Wasserstein distanceë¡œ spectrum distribution ë§¤ì¹­',
            'rationale': 'ì ˆëŒ€ê°’ì´ ì•„ë‹Œ ë¶„í¬ ìœ ì‚¬ì„± í•™ìŠµìœ¼ë¡œ ë” robustí•œ í•™ìŠµ',
            'formula': 'L = W(pred_dist, target_dist) + 0.1 * Pearson correlation',
            'expected_benefit': 'ë¶„í¬ ìˆ˜ì¤€ì˜ ë§¤ì¹­ìœ¼ë¡œ ë” ì•ˆì •ì  í•™ìŠµ',
            'implementation_file': 'losses/wasserstein_spectral.py',
            'config_changes': {
                'loss': {
                    'name': 'wasserstein_spectral',
                    'params': {'num_bins': 30, 'correlation_weight': 0.1}
                }
            }
        })

    # ì œì•ˆ 5: Multi-scale Loss - Frequency domainê¹Œì§€ ê³ ë ¤
    proposals.append({
        'type': 'loss',
        'name': 'MultiScaleSpectralLoss',
        'description': 'ì›ë³¸ + smoothed versionsì— ëŒ€í•´ ë™ì‹œ í•™ìŠµ',
        'rationale': 'ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì—ì„œì˜ ì •í™•ì„± ë™ì‹œ ë‹¬ì„±',
        'formula': 'L = MSE(pred, target) + 0.5*MSE(smooth(pred), smooth(target)) + 0.5*Pearson',
        'smoothing': 'ê°€ìš°ì‹œì•ˆ í•„í„°ë¡œ bin dimension smoothing',
        'expected_benefit': 'ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì—ì„œ ê· í˜•ì¡íŒ í•™ìŠµ',
        'implementation_file': 'losses/multiscale_spectral.py',
        'config_changes': {
            'loss': {
                'name': 'multiscale_spectral',
                'params': {'smooth_kernel': 3, 'pearson_weight': 0.5}
            }
        }
    })

    return proposals


def generate_proposal_summary(model_proposals: List[Dict], loss_proposals: List[Dict]) -> str:
    """ì œì•ˆ ìš”ì•½ ìƒì„±"""
    summary = "\n" + "=" * 100 + "\n"
    summary += "ğŸ¯ CREATIVE MODEL & LOSS FUNCTION PROPOSALS\n"
    summary += "=" * 100 + "\n"

    summary += "\nğŸ“Š NEW MODEL ARCHITECTURES:\n"
    summary += "-" * 100 + "\n"
    for i, prop in enumerate(model_proposals, 1):
        summary += f"\n{i}. {prop['name']}\n"
        summary += f"   ğŸ“ {prop['description']}\n"
        summary += f"   ğŸ’¡ Why: {prop['rationale']}\n"
        summary += f"   ğŸ“ˆ Expected: {prop['expected_benefit']}\n"
        summary += f"   ğŸ“„ File: {prop['implementation_file']}\n"

    summary += "\n\nğŸ’” NEW LOSS FUNCTIONS:\n"
    summary += "-" * 100 + "\n"
    for i, prop in enumerate(loss_proposals, 1):
        summary += f"\n{i}. {prop['name']}\n"
        summary += f"   ğŸ“ {prop['description']}\n"
        summary += f"   ğŸ’¡ Why: {prop['rationale']}\n"
        summary += f"   ğŸ“ Formula: {prop['formula']}\n"
        summary += f"   ğŸ“ˆ Expected: {prop['expected_benefit']}\n"
        summary += f"   ğŸ“„ File: {prop['implementation_file']}\n"

    summary += "\n\n" + "=" * 100 + "\n"
    summary += "ğŸš€ NEXT STEPS:\n"
    summary += "=" * 100 + "\n"
    summary += """
1. ê´€ì‹¬ìˆëŠ” ëª¨ë¸/ì†ì‹¤ ì„ íƒ
2. í•´ë‹¹ êµ¬í˜„ íŒŒì¼ ì‘ì„± (ëª¨ìŠµ: models/*.py ë˜ëŠ” losses/*.py)
3. Config íŒŒì¼ ìƒì„± (test_config_YYYY.yaml)
4. í•™ìŠµ ì‹¤í–‰: python CR_recon/train.py --config test_config_YYYY.yaml
5. Loss ê°’ ë¹„êµ ë° ì±„íƒ

ğŸ’¡ íŒ: ì—¬ëŸ¬ ì¡°í•©ì„ ë™ì‹œì— í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš” (e.g., ìƒˆ ëª¨ë¸ + ìƒˆ ì†ì‹¤í•¨ìˆ˜)
"""

    return summary


def create_implementation_stubs(proposals: List[Dict], output_dir: Path):
    """êµ¬í˜„ ìŠ¤í… ìƒì„± (ì‹¤ì œ êµ¬í˜„ì€ ì‚¬ìš©ìê°€ í•˜ë„ë¡)"""
    print("\nğŸ“ Generating implementation stubs...\n")

    for proposal in proposals:
        file_path = output_dir / proposal['implementation_file']
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if file_path.exists():
            print(f"  âœ“ {proposal['implementation_file']} (already exists)")
            continue

        if proposal['type'] == 'model':
            stub = _create_model_stub(proposal)
        else:
            stub = _create_loss_stub(proposal)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(stub)
        print(f"  âœ“ {proposal['implementation_file']} created")


def _create_model_stub(proposal: Dict) -> str:
    """ëª¨ë¸ êµ¬í˜„ ìŠ¤í… ìƒì„±"""
    return f'''"""
{proposal['name']}: {proposal['description']}

ì•„í‚¤í…ì²˜:
{proposal['architecture_sketch']}

ê¸°ëŒ€ íš¨ê³¼: {proposal['expected_benefit']}
"""
import torch
import torch.nn as nn
import torch.nn.functional as F


class {proposal['name']}(nn.Module):
    """
    {proposal['description']}

    ì…ë ¥: (B, 1, 128, 128)
    ì¶œë ¥: (B, 2, 2, 30)
    """

    def __init__(self, out_len=30, d_model=192, **kwargs):
        """
        Args:
            out_len: Output spectrum bins
            d_model: Feature dimension
            **kwargs: Additional parameters from config
        """
        super().__init__()
        self.out_len = out_len
        self.d_model = d_model

        # TODO: ì•„í‚¤í…ì²˜ êµ¬í˜„
        # {proposal['name']}ì˜ ì„¤ê³„ ì² í•™:
        # {proposal['rationale']}

        # Stem (128 â†’ 64)
        # self.stem_conv = ...

        # Backbone stages
        # self.stage1 = ...
        # self.stage2 = ...
        # self.stage3 = ...
        # self.stage4 = ...

        # Head
        # self.head = ...

        raise NotImplementedError(f"{{self.__class__.__name__}} êµ¬í˜„ í•„ìš”")

    def forward(self, x):
        """
        x: (B, 1, 128, 128)
        Returns: (B, 2, 2, out_len)
        """
        # TODO: Forward pass êµ¬í˜„
        raise NotImplementedError()
'''


def _create_loss_stub(proposal: Dict) -> str:
    """ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ ìŠ¤í… ìƒì„±"""
    return f'''"""
{proposal['name']}: {proposal['description']}

ê³µì‹: {proposal['formula']}

ê¸°ëŒ€ íš¨ê³¼: {proposal['expected_benefit']}
"""
import torch
import torch.nn as nn
import torch.nn.functional as F


class {proposal['name']}(nn.Module):
    """
    {proposal['description']}
    """

    def __init__(self, **kwargs):
        """
        Args:
            **kwargs: Loss parameters from config
        """
        super().__init__()
        # TODO: íŒŒë¼ë¯¸í„° ì €ì¥
        # self.param1 = kwargs.get('param1', default_value)
        raise NotImplementedError(f"{{self.__class__.__name__}} êµ¬í˜„ í•„ìš”")

    def forward(self, pred, target):
        """
        Args:
            pred: (B, 4, out_len) ë˜ëŠ” (B, 2, 2, out_len)
            target: predì™€ ë™ì¼í•œ shape

        Returns:
            loss: scalar
        """
        # TODO: ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
        # ê¸°ë³¸ êµ¬ì¡°:
        # 1. predì™€ targetì„ ì›í•˜ëŠ” shapeë¡œ reshape
        # 2. ì†ì‹¤ ê³„ì‚° (ì˜ˆ: {proposal['formula']})
        # 3. ê²°ê³¼ ë°˜í™˜

        raise NotImplementedError()
'''


def print_analysis(log_data: Dict[str, Any], analysis: Dict[str, Any]):
    """í˜„ì¬ ì„±ëŠ¥ ë¶„ì„ ì¶œë ¥"""
    print("\n" + "=" * 100)
    print("ğŸ“Š CURRENT PERFORMANCE ANALYSIS")
    print("=" * 100)

    print(f"\nğŸ¯ Current Configuration:")
    print(f"  Model: {log_data['model_name']}")
    print(f"  Loss: {log_data['loss_name']}")
    print(f"  Best Val Loss: {log_data['best_val_loss']:.6e}")
    print(f"  Epochs Trained: {log_data['epochs_trained']}")

    print(f"\nğŸ“ˆ Performance Metrics:")
    print(f"  Improvement Rate: {analysis['metrics'].get('improvement_rate', 0):.2%}")
    print(f"  Convergence Trend: {analysis['metrics'].get('convergence_trend', 'unknown')}")
    print(f"  Train-Val Gap: {analysis['metrics'].get('train_val_gap', 0):.2%}")
    print(f"  Overfitting Status: {analysis['metrics'].get('overfitting', 'unknown')}")


def main():
    parser = argparse.ArgumentParser(description='ì°½ì˜ì  ëª¨ë¸/ì†ì‹¤ í•¨ìˆ˜ ì œì•ˆ')
    parser.add_argument('--log', required=True, help='í›ˆë ¨ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--base-config', default='CR_recon/configs/default.yaml',
                        help='Base config íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()

    # ë¡œê·¸ íŒŒì‹±
    print(f"\nğŸ“– Parsing log file: {args.log}")
    log_data = parse_train_log(args.log)

    # ì„±ëŠ¥ ë¶„ì„
    print(f"ğŸ“Š Analyzing performance...")
    analysis = analyze_performance(log_data)

    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print_analysis(log_data, analysis)

    # ì°½ì˜ì  ì œì•ˆ ìƒì„±
    print(f"\nğŸ’¡ Generating creative proposals...")
    model_proposals = propose_creative_models(log_data, analysis)
    loss_proposals = propose_creative_losses(log_data, analysis)

    # ì œì•ˆ ìš”ì•½ ì¶œë ¥
    summary = generate_proposal_summary(model_proposals, loss_proposals)
    print(summary)

    # êµ¬í˜„ ìŠ¤í… ìƒì„±
    output_dir = Path('CR_recon')
    create_implementation_stubs(model_proposals + loss_proposals, output_dir)

    # ì „ì²´ ì œì•ˆ ë‚´ìš©ì„ JSONìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ ì°¸ê³ ìš©)
    proposals_json = {
        'log_analysis': {
            'model': log_data['model_name'],
            'loss': log_data['loss_name'],
            'best_val_loss': float(log_data['best_val_loss']),
            'epochs_trained': log_data['epochs_trained'],
        },
        'analysis': analysis,
        'model_proposals': model_proposals,
        'loss_proposals': loss_proposals,
    }

    proposals_file = Path('CR_recon/outputs/creative_proposals.json')
    proposals_file.parent.mkdir(parents=True, exist_ok=True)
    with open(proposals_file, 'w', encoding='utf-8') as f:
        json.dump(proposals_json, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Proposals saved to: {proposals_file}")


if __name__ == '__main__':
    main()
